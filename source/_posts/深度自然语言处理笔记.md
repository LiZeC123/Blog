---
title: 深度自然语言处理笔记
date: 2020-02-05 14:13:09
categories:
tags:
    - 自然语言处理
cover_picture: images/ml.jpg
math: true
---


本文是斯坦福大学的深度自然语言处理课程的笔记, 可以在B站查看此课程的[全部视频](https://www.bilibili.com/video/av76072224). 这里先随便写一点自己的理解.


word2vec
--------------

核心思想: "A word's meaning is given by the words that frequently appear close-by." 即对于每个词, 周围的几个词应该和这个词具有相近的含义, 因此他们的词向量应该接近. 实现这个目的, 有如下的步骤

1. 通过两个词向量的点乘来表示两者的相识度
2. 通过softmax函数将相似度转化为概率分布
3. 将似然函数最大作为优化目标, 使用梯度下降优化词向量


其中使用softmax转为为概率分布的公式为

$$P(o|c)= \frac{exp(v_0^T)}{\sum_{w=1}^{u} exp(u_w^Tv_c)}$$

从而有目标函数为

$$J(\theta)=-\frac{1}{T}\sum_{1}^{T}\sum_{-m \leq j\leq m} log P(w_{t+j}|w_t)$$

> 注意第二个求和中有\\( j \neq 0 \\)

之后如何求梯度就是各个计算框架的问题了.

### 减少计算量的优化

由于每次计算时需要求解整个语料库的点乘, 而实际只更新窗口中的几个词, 因此可以考虑不计算整个语料库, 而是每次计算时, 随机从语料库中抽取若干词汇, 优化目标函数使这些词汇与中心词的点乘尽量小. 即

$$J(\theta)=log\sigma(u_o^Tv_c)+\sum_{j\sim P(w)}(log\sigma(-u_j^Tv_c))$$

通过优化此目标函数, 可以保证中心词周围的词汇的词向量与中心词接近, 随机抽取的其他词与中心词的词向量不接近.

> 通过P实现按频率随机选择词汇, 可以使用3/4次方来抑制抽到停止词的概率

### GloVe

GloVe的主要思想先读取语料库获得计数矩阵. 然后通过构造目标函数来优化计数矩阵. 计数矩阵统计了每个词周围的词出现的次数, 可以直接粗略的将每一列作为相应词的词向量. GloVe的目标函数定义为

$$J(\theta)= \frac{1}{2} \sum_{i,j=1}^{w}f(P_{ij})(w_i^Tv_j-logP_{ij})^2$$

GloVe模型是一种常见的预训练词向量模型, 更多细节可以参考下面的文章

- [GloVe模型总结](https://zhuanlan.zhihu.com/p/58389508)


### Softmax分类器


定义数据集为 \\( \\{ x_i,y_i \\}^N \\) 其中\\( x_i \\)是输入, 可以是一个词向量. \\( y_i \\)是输出, 可以是一个标签. Softmax分类器的定义为

$$p(y|x)=\frac{exp(W_y x)}{\sum_{c=1}^{C}exp(W_c x)}=softmax(f)_y$$

其中W是一个权重矩阵, 是需要训练的参数. C是标签(分类)的数量. 其中有

$$W_y x = \sum_{i=1}^d w_{yi}x_{i}=f_y$$

> \\(f_y\\)就是将矩阵乘法看做函数的一种表现

相应的目标函数定义为

$$-logP(y|x) = -log\left(\frac{exp(f_y)}{\sum_{c=1}^C exp(f_c)}\right)$$

> 目标函数的定义可以从交叉熵的表达式推导

设p为目标概率分布. 即一个正确分类为1, 其他分类为0的概率分布. 设q为通过softmax计算出的概率分布. 则交叉熵定义为

$$H(p,q)=-\sum_{c=1}^C p(c)logq(c)$$

> p只有正确的分为值为1, 因此就是正确分类的softmax值的负对数

最终\\( J(\theta)\\)定义为所有样本数据的平均值

$$J(\theta)=\frac{1}{N} \sum_{i=1}^N -log\left(\frac{exp(f_y)}{\sum_{c=1}^C exp(f_c)}\right)$$

此时 \\( \theta = W \in R^{Cd} \\), 如果同时调整词向量的取值, 则\\( \theta \in R^{Cd + Vd} \\). 由于Vd是一个较大的值, 因此模型的参数数量显著增加, 如果数据集数量较少, 容易导致模型欠拟合.


### Window Classification

Window Classification是一个分类任务, 其核心思想是提取中心词周围的若干词汇, 构成一个拼接的词向量, 然后使用这个词向量训练一个softmax分类模型.

设\\( \hat{y} \\) 为通过softmax计算的概率分布, \\( t \\)为目标分布, 则导数可以拆分为对每个分类的导数.

$$\frac{\partial }{\partial x} = -log softmax(f_y(x))=\sum_{c=1}^C - \frac{\partial log softmax(f_y(x))}{\partial f_c} \frac{\partial f_c(x)}{\partial x}$$

此时可以分为c为目标分类(等于y)和c不是目标分类进行讨论. 一波操作后可以得到

$$\frac{\partial }{\partial x} = \begin{bmatrix}
\hat{y_1}
\\\\ 
\vdots 
\\\\ 
\hat{y_y}-1
\\\\ 
\vdots
\\\\ 
\hat{y_c}
\end{bmatrix}
= [\hat{y} - t] = \delta$$

### The Max-Margin Loss

定义\\(s\\)表示正确的分类的得分, \\(s_c\\)表示错误的分类的得分, 则损失函数定义为

$$J=max(0, 1-s+s_c)$$

这里的数字1是一个超参数, 表示当正确的分类和错误的分类的得分差值大于1就可以停止优化. 通过这样的定义,可以使得每次优化时集中优化差距较大的数据, 而足够好的数据就直接忽略.

\\(s_c\\)表示负样本, 可以通过随机采样的方式获取.



循环神经网络模型
-----------------


RNN模型的特点是具有一个隐含层, 从而每一次输出除了和当前的输入相关, 还与隐含层的值相关, 具体定义如下

$$
h_t &= \phi(h_{t-1},x_t) = f(w^{(hh)}h_{t-1}+W^{(hx)}x_t) \\\\

$$


### GRUs模型

GRUs模型相比于RNN模型引入了update和reset机制, 首先定义两个控制向量

$$
\begin{align}
z_t &= \sigma(W^{(z)}x_t + U^{(z)}h_{t-1}) \\\\
r_t &= \sigma(W^{(r)}x_t + U^{(r)}h_{t-1})
\end{align}
$$

其中 \\( z_t \\) 称为Update Gate, \\( r_t \\) 称为Reset Gate. 在此基础上定义

$$
\begin{align}
\tilde{h}_t &= tanh(Wx_t + r_t \circ Uh_{t-1}) \\\\
h_t         &= z_t \circ h_{t-1} + (1-z_t) \circ \tilde{h}_t
\end{align}
$$

其中r用于控制是否忽略以往的取值(即隐含层), 当r为0时, 则与考虑本次输入的影响. z用于控制隐含层的更新情况. 当z取值为1时, 则隐含层取值等于上一次隐含层的取值, 从而可以保存关键信息不被新的输入影响.

### LSTMs模型

LSTMs模型首先定义了Input Gate, Forget Gate和输出, LSTMs模型与GRUs模型很类似, Input Gate 用于控制输入, Forget Gate用于控制隐含层, 三个部分的结构是相同的.

$$
\begin{align}
i_t &= \sigma(W^{(i)}x_t + U^{(i)}h_{t-1}) \\\\
f_t &= \sigma(W^{(f)}x_t + U^{(f)}h_{t-1}) \\\\
o_t &= \sigma(W^{(o)}x_t + U^{(o)}h_{t-1})
\end{align}
$$

然后可以分别定义New Memory Cell, Final Memory Cell和Final Hidden State, 具体如下所示.

$$
\begin{align}
\tilde{c}_t &= tanh(W^{(c)}x_t + U^{(c)}h_{t-1})  \\\\
c_t         &= f_t \circ c_{t-1} + i_t \circ \tilde{c}_t \\\\   
h_t         &= o_t \circ tanh(c_t)
\end{align}
$$

### 注意力机制

由于在执行机器翻译任务是, 会自动时候所有的输入作为参数, 因此注意力机制引入了一个额外的模块, 这个模块对输入的此进行评分, 从而使得网络在翻译的时候可以侧重于某个部分.

### BLUE评分方法

BLUE评分方法是一种评价机器翻译质量的方法. 其核心思想是对于一篇文章, 给定一个人类翻译的版本作为参考, 比较机器翻译的版本与人翻译的版本有多少个词是按照同样的顺序出现. 此外, 为了避免机器翻译只输出一个词, 如果机器翻译的文本长度比人类翻译的文本的长度小, 则会受到的一定程度的惩罚.

通过此方法可以一定程度的体现机器翻译对语法结构的掌握.




递归神经网络模型
------------------------

递归神经网络的核心思想是提供一种将局部模块组合为一个整体的方法


### 短语的词向量

在前面的课程中, 以及学习了如何获得单词的向量, 现在希望对一些短语也能获得相应的向量, 从而能够比较不同短语的含义. 但由于短语的数量远大于单词的数量, 因此无法将所有的短语都存储下来.

一个合理的方法是提供一种合成方法, 从而根据单词的向量合成短语的向量.  通过训练一个神经网络来实现向量的合并同时给出分数来指示这个合并是否正确