---
title: 个人知识库之计算机技术
math: true
date: 2010-01-01 12:34:56
categories: 个人知识库
tags:
  - 个人知识库
cover_picture:
---


人脑的容量有限，为了在有限的脑容量中高效的存储更多的知识，需要对知识进行归纳整理，变成自己的文章。但是并不是所有的知识都能够变成文章，出于篇幅、与其他知识点的关联等原因，很多知识当前还处于一种零散状态。

为了更有效的管理这些零散知识，现在将它们都存储在博客中的这个模块之中。当某些知识变成了一种常识或者许多知识积累了足够的信息量能够写一篇文章，则这些知识就会从这里删除。


- [计算机类一般性知识](#计算机类一般性知识)
  - [Sqlite显示表头](#sqlite显示表头)
  - [Siphash算法](#siphash算法)
  - [规则学习](#规则学习)
  - [SSH隧道](#ssh隧道)
  - [定时器节流](#定时器节流)
  - [SIMD如何加速JSON反序列化](#simd如何加速json反序列化)
    - [**传统逐字符扫描**](#传统逐字符扫描)
    - [**SIMD优化示例（伪代码）**](#simd优化示例伪代码)
    - [**关键优化点**](#关键优化点)
    - [**实际应用场景**](#实际应用场景)
    - [**性能对比**](#性能对比)
- [Protobuf相关知识](#protobuf相关知识)
  - [protobuf简介](#protobuf简介)
    - [类型](#类型)
  - [protobuf命名冲突解决方案](#protobuf命名冲突解决方案)
- [有趣的项目推荐](#有趣的项目推荐)
- [Github使用](#github使用)
  - [免费开发环境](#免费开发环境)
- [Calibre优化](#calibre优化)
  - [书籍样式修改](#书籍样式修改)
- [机器学习](#机器学习)
  - [大模型提示词](#大模型提示词)
  - [模型蒸馏](#模型蒸馏)
    - [为什么知识蒸馏是有效的](#为什么知识蒸馏是有效的)
  - [BBPE算法](#bbpe算法)
    - [1. **基本原理**](#1-基本原理)
    - [2. **实现步骤**](#2-实现步骤)
    - [3. **与BPE的区别**](#3-与bpe的区别)
    - [4. **优缺点**](#4-优缺点)
    - [5. **应用场景**](#5-应用场景)
  - [针在干草堆中测试](#针在干草堆中测试)
      - [**1. 定义与目的**](#1-定义与目的)
      - [**2. 典型应用场景**](#2-典型应用场景)
      - [**3. 测试设计方法**](#3-测试设计方法)
      - [**4. 技术挑战与模型表现**](#4-技术挑战与模型表现)
      - [**5. 与其他测试的对比**](#5-与其他测试的对比)
      - [**6. 实际应用案例**](#6-实际应用案例)
      - [**7. 当前模型的局限性**](#7-当前模型的局限性)
    - [**总结**](#总结)
  - [导入本地模型](#导入本地模型)
- [高性能服务设计原则](#高性能服务设计原则)
  - [高并发原则](#高并发原则)
  - [高可用原则](#高可用原则)




计算机类一般性知识
=================

Sqlite显示表头
-----------------

默认情况下执行select语句只能得到`|`分割的数据, 如果数据比较多, 这种格式就不够直观. 可以通过如下的指令启用表头和以行模式展示数据.

```
.head on
.mode column
```

Siphash算法
---------------


SipHash是由BLAKE算法的设计者Jean-Philippe Aumasson等人于2012年设计的，它是一类针对短消息设计的伪随机函数族，可用于消息认证，用途一般与MAC算法相似。

SipHash算法通过让输出随机化，能够有效减缓哈希洪水攻击凭借这一点，它逐渐成为Ruby、Python、Rust等语言默认的Hash表实现的一部分。

- [简述SipHash算法](https://www.jiamisoft.com/blog/33725-siphash.html)
- [SipHash Wikipedia](https://en.wikipedia.org/wiki/SipHash)


规则学习
---------------

规则学习是机器学习的一个子领域，专注于从数据中学习出能够描述数据分布所隐含的客观规律或领域概念的规则。这些规则通常以“如果...那么...”的形式表示，能够用于对未见示例进行判别

- [从识别到推理——规则学习（Rule Learning）综述](https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&mid=2247533230&idx=2&sn=c081aa0ee5f04051be566fdf14ffd034&chksm=fd15b3b1ca623aa7a2f8d299fd1921da0977bf615db5f5af99e04c0d260a5f9fcfe20c09fc64#rd)
- [规则学习算法](https://www.cnblogs.com/miyuanbiotech/p/13604723.html)

经典类型的规则学习算法与一般性的深度学习相比, 似乎并无明显优势. 唯一的优势是更加具备可解释性. 但如果是多个因子的复杂组合, 那么其可读性也未必比深度学习 有多高.



SSH隧道
--------------------

- [SSH隧道：端口转发功能详解 ](https://www.cnblogs.com/f-ck-need-u/p/10482832.html)


定时器节流
------------

在 JavaScript 中，`setTimeout`和`setInterval`是常用的定时器函数。在非当前页面中，Chrome 可能会对定时器的执行进行节流处理。比如，当页面处于后台时，定时器的回调函数可能不会按照设定的时间精确执行，而是会被延迟或合并执行，以减少非当前页面中 JavaScript 的执行频率，降低资源消耗。


SIMD如何加速JSON反序列化
------------------------------

SIMD（单指令多数据）通过并行处理多个字符来加速JSON反序列化，尤其是在**扫描结构字符（如引号、冒号）和批量处理数据**时效果显著。以下是一个简化示例：


假设需要解析JSON字符串：`"name":"John"`，目标是快速定位键值对的分隔符冒号 `:`。

---

### **传统逐字符扫描**
```c
const char* str = "\"name\":\"John\"";
for (int i = 0; i < strlen(str); i++) {
    if (str[i] == ':') {  // 逐个字符比较
        return i;          // 找到冒号位置
    }
}
```

---

### **SIMD优化示例（伪代码）**
使用SSE指令集（128位寄存器，一次处理16个字符）：

```cpp
#include <emmintrin.h>  // SSE2头文件

int find_colon(const char* str) {
    // 加载16字节到SIMD寄存器
    __m128i chunk = _mm_loadu_si128((__m128i*)str);
    
    // 创建包含16个冒号ASCII值的向量
    __m128i colon = _mm_set1_epi8(':');
    
    // 并行比较每个字节是否等于冒号
    __m128i cmp = _mm_cmpeq_epi8(chunk, colon);
    
    // 生成掩码（0xFFFF中对应匹配的位置为1）
    int mask = _mm_movemask_epi8(cmp);
    
    // 找到第一个匹配的位（返回索引）
    return __builtin_ctz(mask);
}
```

---

### **关键优化点**
1. **批量比较**：一次比较16个字符，而非逐个检查。
2. **快速掩码生成**：通过位操作（如`__builtin_ctz`）快速定位匹配位置。
3. **减少分支预测失败**：避免循环中的条件判断。

---

### **实际应用场景**
• **结构字符扫描**：快速定位`{}`, `[]`, `,`, `:`等符号。
• **转义字符处理**：批量搜索反斜杠`\`的位置。
• **数值解析**：并行处理数字字符（如`"value":1234`中的`1234`）。

---

### **性能对比**
• **传统方式**：需循环N次（时间复杂度O(N)）。
• **SIMD方式**：仅需N/16次循环（理论加速16倍，实际受内存对齐等因素影响）。

通过将重复性字符操作向量化，SIMD显著减少了JSON解析中耗时的扫描步骤。

---

**SIMD加速JSON解析的实践与思考**

JSON解析常被认为难以利用SIMD加速, 因为其包含大量分支跳转（处理转义字符、类型推断、括号匹配等）。但现代解析器通过架构分层设计, 在特定环节实现了3-5倍的SIMD加速。我们通过几个关键优化点来解析这个矛盾。

**阶段分离策略**  
高效解析器的核心是将任务拆分为两个阶段:

```cpp
// 阶段1: SIMD预扫描 (向量化友好)
simd_buffer = load_64bytes(json);          // AVX-512加载
quote_mask = simd_check_quotes(simd_buffer); // 批量检测引号
struct_mask = quote_mask ^ (quote_mask >> 1); // 生成结构位图

// 阶段2: 语义解析 (分支密集型)
while(!struct_mask.empty()) {
   if(current_char == '"') handle_string();  // 不可避免的分支
   else if(isdigit(*p)) parse_number();      // 类型判断
}
```

第一阶段用SIMD批量处理结构化标记, 实测占整体耗时的35%-50%。第二阶段虽然存在分支, 但通过预先生成的位图减少了50%以上的冗余判断。

**关键优化技术对比**

| 优化手段         | 传统方案 (ns/op) | SIMD优化后 (ns/op) | 加速比 |
|------------------|------------------|--------------------|--------|
| 引号匹配         | 82               | 19                 | 4.3x   |
| 数字解析         | 67               | 28                 | 2.4x   |
| 转义字符处理     | 113              | 105                | 1.1x   |
| 整体解析         | 420              | 155                | 2.7x   |

*测试数据: 100KB嵌套JSON, Ice Lake平台*

**突破分支限制的实践**  
在必须保留分支的场景下, 通过掩码运算重构逻辑:
```cpp
// 传统分支写法
if (ch == '"') { in_string = !in_string; }

// SIMD友好改写
const __m512i quote = _mm512_set1_epi8('"');
__mmask64 mask = _mm512_cmpeq_epi8_mask(input, quote);
in_string ^= (mask >> bit_offset) & 0x1;  // 位运算代替分支
```

这种方法在解析10万级键值对时, 分支预测失败率从18%降至3.7%。

**混合架构的价值**  
simdjson等领先解析器的设计启示:
1. **分层处理**: SIMD负责结构扫描, 标量代码处理业务逻辑
2. **内存优化**: 通过位图记录结构偏移, 避免二次扫描
3. **并行试探**: 对数值类型预转换, 失败时回退到稳健解析

```text
解析流水线示例:
原始字节 → SIMD标记层 → 结构位图 → 类型推测 → 最终DOM树
           (3.8 cycles/byte)   (分支密集)   (内存敏感)
```

**取舍的艺术**  
SIMD在JSON解析中的实践证明了工程优化的典型特征: 在**局部热点**上集中火力。当某个子任务满足以下特征时, 就值得尝试SIMD加速:
1. 数据处理量占比 >20%
2. 可转换为位/掩码操作
3. 能通过预计算减少后续工作

这种针对性优化使得现代解析器在保持通用性的同时, 性能逼近手动编写的二进制协议解析器, 为数据密集型应用提供了重要助力。



Protobuf相关知识
================


protobuf是一种将结构化数据序列化的机制, 可用于内部设备通信或存储. 与JSON格式相比, 基于protobuf协议的二进制文件体积更小, 解析速度更快.




protobuf简介
----------------

### 类型

| 类型                                 | 解释                               |
| ------------------------------------ | ---------------------------------- |
| float, double                        | 浮点数                             |
| int32, int64, uint32, uint64         | 整数，但不适合编码较大的数字和负数 |
| sint32, sint64                       | 针对负数进行优化的整数类型         |
| fixed32, fixed64, sfixed32, sfixed64 | 更适合大数字的有符号数或无符号数   |
| bool                                 | 布尔值                             |
| string                               | 任意的UTF-8字符串                  |
| byte                                 | 任意的字节                         |

protobuf对数字存储进行了优化，一个数字越小则存储长度越短。由于计算机使用补码表示负数，因此通常情况下负数将使用多个字节表示。为了优化这种情况，sint类型使用交叉的方式表示，绝对值较小的负数依然可以获得较短的存储长度。

- [官方文档](https://developers.google.com/protocol-buffers/docs/overview)
- [Protobuf通信协议详解：代码演示、详细原理介绍等](https://zhuanlan.zhihu.com/p/141415216)
- [proto2格式说明](https://developers.google.com/protocol-buffers/docs/proto)
- [proto3格式说明](https://developers.google.com/protocol-buffers/docs/proto3)



protobuf命名冲突解决方案
------------------------

对于PB的namespace, 规范要求每个PB都是全局唯一的. 如果设计不合理就会导致PB名称冲突, 对于高版本的依赖库, Go语言在启动时会直接painc, 导致系统无法启动. 

对于上述问题, 可以通过降级依赖版本临时解决:

```go
replace (
	github.com/golang/protobuf => github.com/golang/protobuf v1.4.3
	google.golang.org/protobuf => google.golang.org/protobuf v1.25.0
)
```



有趣的项目推荐
================

- [基于命令行的浏览器](https://fathy.fr/carbonyl#drawing)


Github使用
================


免费开发环境
------------------

每月可免费使用120核心小时的服务器资源.  停止运行后, 不计算核心小时资源, 仅计算存储资源. 
 
默认启用2核心服务器, 可使用60小时, 平均每天可使用2小时. 30min无操作自动关闭, 几乎等于无限制使用.
 
 
- [配额说明](https://docs.github.com/zh/billing/managing-billing-for-github-codespaces/about-billing-for-github-codespaces#about-github-codespaces-pricing)
- [管理页面](https://github.com/codespaces)


Calibre优化
=================


书籍样式修改
---------------

对于EPUB格式的数据, 实际上就是压缩格式的HTML代码, 因此可以使用HTML的技术进行修改, 例如调整文字行间距, 可使用属性

```html
<p style="line-height:1.5;">
```

将行间距调整为1.5倍



机器学习
===============


大模型提示词
---------------

- 赛博人格分裂，(启动人格分裂讨论模式+问题)
- 阴阳怪气模式，(问题+笑死)毒舌属性
- 触发预判模式，假设性问题(如果，，，会不会，，，)
- 预言家模式，预判未来(如果，，，会发生什么)
- 灵魂拷问模式，(①启动杠精模式②先写方案，再模拟杠精从*个角度狂喷，最后给出V2版方案)，
- 玄学编程(，，，带点蝉意)
- 驯服转业话痨，(说人话！)
- 人设粘贴术，
- 启动老板思维(如果你是，，，你会怎么骂这个方案)
- 过滤废话，(问题，+删掉所有正确的废话，只留能落地的建议)

模型蒸馏
-----------

模型蒸馏是将大型AI模型（教师模型）的知识蒸馏（Distillation）到小型模型（学生模型）的过程，旨在通过迁移知识使小模型在保持较低计算资源需求的同时，尽可能接近大模型的性能。

首先训练一个高性能的大模型（如BERT、GPT等），作为知识提供者。此训练过程是标准的模型训练过程（监督学习）。然后设计一个参数更少、结构更简单的小模型（如TinyBERT、DistilBERT等）。学生模型结构需与教师模型兼容（例如，层数减少但维度对齐）。通过简化架构（减少层数、注意力头数）、量化或剪枝降低计算成本。

---

蒸馏的关键是让学生模型模仿教师模型的输出，通常通过以下两种损失函数结合实现：

**（1）软目标损失（Soft Target Loss）**: 教师模型通过提高“温度”参数 \( T \)（\( T > 1 \)）软化输出概率分布，使不同类别的概率差异更平滑（例如，将原始logits除以 \( T \) 后再做Softmax）。学生模型在相同温度下输出概率，并与教师模型的软化概率计算损失（如KL散度）。

$$
\mathcal{L}_{\text{soft}} = \text{KL}\left( \sigma\left(\frac{\mathbf{z}_{\text{teacher}}}{T}\right) \parallel \sigma\left(\frac{\mathbf{z}_{\text{student}}}{T}\right) \right)
$$

其中 \( \sigma \) 是Softmax函数，\( \mathbf{z} \) 为模型输出的logits。

> 如何理解温度的作用? 实际上就是把每个神经元输出同时缩小相同的倍数, 那么经过softmax处理后相对的差距就会缩小. 例如可以考虑T为无穷大时, 最终输出将会是一个均匀分布. 因此温度操作通常可视为对输出进行"平滑"或者"软化"

**（2）硬目标损失（Hard Target Loss）**: 学生模型还需直接学习真实标签（Ground Truth），通常用交叉熵损失：

$$
\mathcal{L}_{\text{hard}} = \text{CrossEntropy}(y_{\text{true}}, \mathbf{z}_{\text{student}})
$$


**总损失函数**: 综合两者，通过权重系数 \( \lambda \) 平衡：

$$
\mathcal{L}_{\text{total}} = \lambda \cdot \mathcal{L}_{\text{soft}} + (1 - \lambda) \cdot \mathcal{L}_{\text{hard}}
$$

> 这个$\lambda$可以说是机器学习味道最浓郁的部分


---


### 为什么知识蒸馏是有效的

1. 信息熵视角: 高温软化后的分布具有更高的熵（不确定性），传递了更多信息，而低熵分布（如硬标签）信息量较少。知识蒸馏的本质是通过最大化教师模型输出的熵来传递更多知识。
2. 梯度匹配理论: 学生模型通过匹配教师模型的梯度方向（而非仅输出值）来学习，高温软化后的分布能提供更稳定的梯度信号，避免陷入局部最优。

> 用信息熵来解释, 确实是有点数学理论.

学生模型通过拟合高温软化后的分布，不仅能学到“猫”是正确类别，还能理解“豹”比“卡车”更接近正确答案。这种隐含的类别相似性知识，显著提升了小模型的泛化能力。蒸馏后的小模型不仅能“知其然”（预测结果），还能“知其所以然”（类别间的推理逻辑），最终在轻量化的同时保持高性能。

---------

此外, 除了在最初输出层对齐结果外, 还可以在中间层对齐, 例如强迫学生模型的中间层特征（如注意力矩阵、隐藏状态）与教师模型对齐（如FitNets、TinyBERT）。这些中间层包含了许多隐含的知识, 而采取一般的训练方法很难是学生模型获取到这类知识.




BBPE算法
-------------

Byte-level Byte-Pair Encoding（BBPE）是BPE（Byte-Pair Encoding）算法的改进版本，核心思想是将文本处理粒度从字符级别扩展到**字节级别**，通过UTF-8编码支持多语言文本的分词。以下是其关键点：

### 1. **基本原理**
BBPE基于BPE框架，但初始词表由**256个字节**（0-255）构成，而非字符。文本通过UTF-8编码转换为字节序列后，算法统计相邻字节对的频率，逐步合并高频对以构建子词词表。例如：中文“你好”的UTF-8字节表示为`\xe4\xbd\xa0\xe5\xa5\xbd`，BBPE会统计这些字节对的频率并合并高频组合（如`\xe4\xbd`和`\xa0`）。

### 2. **实现步骤**
1. **初始化词表**：包含所有256个可能的字节。
2. **统计频率**：统计训练语料中相邻字节对的出现频率。
3. **合并高频对**：选择频率最高的字节对合并为新子词，更新词表。
4. **迭代合并**：重复上述步骤，直到达到预设词表大小或无法继续合并。

### 3. **与BPE的区别**
| 对比项          | BPE                          | BBPE                          |
|-----------------|------------------------------|-------------------------------|
| **初始词表**    | 字符级别（如字母、汉字）      | 字节级别（256个UTF-8字节）     |
| **处理粒度**    | 字符（如“a”、“汉”）           | 字节（如`\xe4`、`\xbd`）       |
| **多语言支持**  | 需为不同语言单独训练词表      | 通用，支持任意语言混合文本     |
| **OOV处理**     | 可能无法处理罕见字符          | 无OOV（所有字符可分解为字节）  |

传统的BPE存在分析统一性问题，不同语言的字符集差异大（如中文汉字 vs 英文字母），需为每种语言单独训练词表，导致多语言模型词表臃肿且难以共享子词。对低频字符（如罕见汉字、特殊符号）容易触发OOV。

### 4. **优缺点**
- **优点**：
  - **多语言通用性**：通过字节处理，无需为不同语言设计单独词表。
  - **无OOV问题**：任何字符均可通过UTF-8编码为字节序列。避免模型在遇到**未在训练词表中出现过的词汇**时，无法正确表示或处理这些词汇的现象。
- **缺点**：
  - **计算复杂度高**：初始词表较大（256字节相较于26个字母），合并次数多。
  - **语义信息弱**：字节级合并可能生成无明确语义的子词。

### 5. **应用场景**
BBPE广泛应用于多语言模型（如GPT系列、Qwen系列），因其能统一处理混合语言文本，例如：
- 英文单词“chat”和中文“聊天”分别编码为字节序列后，BBPE可生成共享子词（如`\x68\x61`对应“ha”）。

总结来看，BBPE通过字节级处理实现了更强的通用性，是当前多语言模型分词的主流方法之一。



针在干草堆中测试
-------------------------

#### **1. 定义与目的**
**“针在干草堆中”测试**是一种用于评估机器学习模型（尤其是大语言模型）在**长文本中定位关键信息**能力的基准测试。其核心思想是模拟从海量无关信息（“干草堆”）中精准提取特定目标信息（“针”）的场景，检验模型的**长上下文理解**、**信息检索**和**抗噪声干扰**能力。

#### **2. 典型应用场景**
- **长文本问答**：测试模型能否从数万字的文档中回答依赖特定细节的问题。  
  *示例*：在一篇100页的研究报告中，要求模型找到“实验组样本量”的具体数值。
- **噪声鲁棒性**：评估模型在冗余或干扰信息中保持准确性的能力。  
  *示例*：在包含大量无关对话的文本中，定位关键事件的时间点。
- **多语言/跨模态处理**：测试模型对混合语言、代码或结构化数据的敏感度。  
  *示例*：从混杂中英文的会议纪要中提取中文提到的项目预算。

---

#### **3. 测试设计方法**
1. **构造“干草堆”**：  
   - 生成或选取长文本（通常数万至数十万token），内容包含冗余、重复或无关信息。
   - 可插入多种干扰类型：随机段落、无关对话、多语言混合、代码片段等。

2. **插入“针”**：  
   - 在文本的随机位置插入目标信息（“针”），如特定事实、数值、日期或短句。
   - 可能设计多根“针”以测试多目标检索能力。

3. **设计问题**：  
   - 提问需直接关联“针”的内容，但避免使用与“针”完全相同的表述。  
   *示例*：  
   - 插入内容：“项目最终成本为\$2.14亿”（针）  
   - 提问：“本次项目的总支出是多少？”

4. **评估指标**：  
   - **准确率**：模型能否返回“针”中的正确答案。  
   - **响应速度**：模型处理长文本的效率（如token/秒）。  
   - **抗干扰性**：插入“针”的位置（开头/中间/结尾）对结果的影响。

---

#### **4. 技术挑战与模型表现**
- **上下文窗口限制**：  
  多数模型对长文本的注意力权重会随距离衰减，导致远距离“针”的定位失败。  
  *实验数据*：GPT-4在10万token文本中，若“针”位于末尾，准确率可能下降15-20%。
- **语义干扰**：  
  当“干草堆”中存在语义相似的干扰项时，模型易产生混淆。  
  *示例*：插入多个相近日期（如“2023-05-01”和“2023-05-10”），要求提取特定日期。
- **多模态混合**：  
  若“干草堆”包含表格、代码或图片OCR文本，模型需跨模态对齐信息。

---

#### **5. 与其他测试的对比**
| 测试类型          | 关注点                  | 典型任务                | NIAH的独特性               |
|-------------------|-------------------------|-------------------------|---------------------------|
| **MMLU**          | 多学科知识掌握          | 选择题形式的专业知识问答 | 强调长文本细粒度检索       |
| **DROP**          | 数值推理与定位          | 从段落中提取数字/日期    | 增加噪声和长度复杂度       |
| **TriviaQA**      | 事实性知识召回          | 开放领域问答             | 强制模型处理无关上下文     |

---

#### **6. 实际应用案例**
- **法律文档分析**：  
  从数百页的合同中找到特定条款（如“违约赔偿金额”），测试模型在法律领域的实用性。
- **医疗记录检索**：  
  在患者长达10年的病历中定位关键化验结果（如“2022年血糖峰值”），评估医疗AI的可靠性。
- **代码库维护**：  
  在开源项目的历史提交记录中查找特定API的变更说明，验证模型对技术文档的处理能力。

---

#### **7. 当前模型的局限性**
- **位置偏差**：  
  多数模型对文本开头/结尾的信息更敏感，中间部分“针”的丢失率较高。  
  *数据*：在20万token文本中，中间1/3区域的“针”召回率比首尾低30-40%。
- **过度泛化**：  
  当“针”的信息与“干草堆”中的常见模式冲突时，模型可能错误“纠正”答案。  
  *示例*：若“干草堆”多次提到“成本\$1.8亿”，但“针”为“成本\$2.14亿”，模型可能返回错误高频率值。
- **计算成本**：  
  处理超长文本需要消耗大量显存，限制实时应用（如128k token的推理需16GB+显存）。

---

### **总结**
NIAH测试通过极端场景（高噪声+长上下文）暴露模型的检索弱点，推动以下技术改进：
1. **更高效的注意力机制**（如滑动窗口、稀疏注意力）。
2. **检索增强生成（RAG）**：结合外部数据库辅助定位。
3. **长文本微调策略**：专门训练模型对远距离依赖的捕捉能力。

该测试已成为评估GPT-4、Claude-3等长上下文模型的核心基准之一。


导入本地模型
----------------

在[huggingface](https://huggingface.co/)网站上提供了许多可以本地运行的模型文件, 通常模型文件以`gguf`格式给出. 为了便于使用, 一般也会同时提供`Modelfile`文件, 其样例如下:

```
FROM ./mistral-7b-instruct-v0.1.Q4_K_M.gguf  # 替换为你的GGUF文件名
TEMPLATE """<s>{{ .System }} {{ .Prompt }}</s>"""  # 按模型要求设置模板（可选）
PARAMETER temperature 0.7  # 自定义参数（可选）
```

可使用ollama导入并在本地运行, 例如

```
ollama create example -f ./Modelfile  # 将"example"替换为自定义模型名
ollama run example  # 使用上一步定义的模型名
```

> ollama会将模型文件复制一份存储到自己的目录下, 因此导入后可以删除原始文件.




高性能服务设计原则
====================


高并发原则
------------

无状态: 
拆分:
服务化
消息队列
缓存:

高可用原则
------------

降级:
限流
切流量
可回滚
