---
title: 机器学习之强化学习
date: 2020-06-26 12:45:16
categories: 机器学习
tags:
    - 机器学习
cover_picture: images/ml.jpg
---

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>


强化学习(Reinforcement Learning, RL), 也叫增强学习, 是指一类从(与环境)交互中不断学习的问题以及解决这类问题的方法. 强化学习的关键是贡献度分配问题, 每一个动作不能直接得到监督信息, 需要通过最终结果获得监督信息, 并且存在延时.

基础概念
--------------

强化学习涉及如下的一些基础概念
1. 智能体(agent)可以感知外界环境的状态(state)和反馈的奖励(reward),  并进行学习和决策
2. 智能体的决策功能是指根据外界环境的状态来做出不同的动作(action), 而学习功能是指根据外界环境的奖励来调整策略
3. 环境(environment)是智能体外部的所有事物, 并受智能体动作的影响而改变其状态, 并反馈给智能体相应的奖励

在此基础上, 可以抽象出如下的一些概念

名称            | 符号      | 备注
----------------|-----------|----------------------------------------------------------
状态            | s         | 对环境的描述/ 可以是离散的或连续 / 状态空间为S
动作            | a         | 对智能体行为的描述 / 可以是离散的或连续 / 动作空间为A
策略            |π(a&#124;s)     | 智能体根据环境状态s 来决定下一步的动作a 的函数
状态转移概率    | p(s′&#124;s, a) | 智能体做出一个动作, 环境在下一个时刻转变为状态s′ 的概率
即时奖励        |r(s, a, s′) | 智能体做出动作之后, 环境反馈给智能体的奖励(标量)

策略可以分为确定性策略和随机性策略, 其中随机性策略表示在某个状态下, 智能体选择的动作服从某一个概率分布而不是确定动作. 强化学习一般采用随机性策略.

### 回报与折扣率

给定一个策略后, 智能体收到的所有即时回报之和为总回报(Return). 可以引入一个折扣率来综合考虑近期回报与远期回报, 其定义为

$$G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}$$

其中 \\( \gamma^{t} \\) 表示折扣率. \\( \tau \\) 表示马尔可夫决策过程的一个轨迹, 即由s,a和r构成的一组序列, 表示了智能体在环境中的一系列操作下, 相应的环境变换和奖励.


### 目标函数与值函数

强化学习的目标是获得一个策略, 来最大化期望回报, 即

$$\mathcal{J}(\theta)=E_{\tau \sim p_{\theta}(\tau)}[G(\tau)]=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}\right]$$

其中\\( p_{\theta}(\tau) \\)表示一个通过参数\\(\theta\\)调节分布, 而\\(\tau \sim p_{\theta}(\tau)\\) 表示轨迹服从这一分布, 最终这个公式的含义就是此分布下求解期望值. 

上述公式可以分解为

$$\\begin{aligned}E_{\\tau \\sim p(\\tau)}[G(\\tau)] &=E_{s \\sim p\\left(s_{0}\\right)}\\left[E_{\\tau \\sim p(\\tau)}\\left[\\sum_{t=0}^{T-1} \\gamma^{t} r_{t+1} \\mid \\tau_{s_{0}}=s\\right]\\right] \\\\&=E_{s \\sim p\\left(s_{0}\\right)}\\left[V^{\\pi}(s)\\right]\\end{aligned}$$

其中\\( V^{\pi}(s) \\)称为状态值函数(State Value Function), 表示从状态s开始, 执行策略π 得到的期望总回报. 经过一番推导, 可以得到如下的贝尔曼方程, 按照这种方程计算的方法也称为动态规划法.

$$V^{\pi}(s)=E_{a \\sim \\pi(a \\mid s)} E_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$

通过此公式可以使用迭代法计算状态值函数. 如果定义状态-动作值函数为

$$Q^{\\pi}(s, a)=E_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$

则可以进一步将上述公式表示为状态值函数中\\( V^{\pi}(s) \\)是Q函数\\(Q^{\\pi}(s, a)\\)关于动作a的期望, 即

$$V^{\pi}(s)=E_{a \sim \pi(a \mid s)}\left[Q^{\pi}(s, a)\right]$$

总结两种值函数, 即
- \\(V^{\pi}(s)\\): 在状态s下, 以策略π可以获得的总期望回报
- \\(Q^{\\pi}(s, a)\\): 在状态s下, 选择动作a后, 以策略π可以获得的总期望回报

值函数可以看作是对策略π的评估. 如果在状态s, 有一个动作a使得\\( Q^{\\pi}(s, a) > V^{\pi}(s)\\), 则说明执行动作a 比当前的策略要好, 我们就可以调整参数使得动作a的概率增加.



深度强化学习
---------------


深度强化学习是指将强化学习与深度学习结合的方法. 用强化学习来定义问题和优化目标, 用深度学习来解决策略和值函数.

### 蒙特卡洛方法

模型无关的强化学习使用采样的方法计算值函数, 在这种情况下, 往往无法得知状态转移概率和及时奖励, 因此首先以状态s和动作a开始, 随机执行若干次采样过程, 获取一组轨迹, 则Q函数近似为这组轨迹的总回报的平均值

$$Q^{\\pi}(s, a) \\approx \\hat{Q}^{\\pi}(s, a)=\\frac{1}{N} \\sum_{n=1}^{N} G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\right)$$

获得近似的Q函数以后, 就可以执行策略迭代过程, 即令

$$\forall s, \pi(s)=\arg \max _{a} Q(s, a)$$

------------------

在上述蒙特卡洛方法中, 如果是确定性策略, 那么实际上采样过程获取的样本都是一样的, 并不能获取不同样本. 因此可以引入环境探索率ϵ, 使得再采样过程中, 以ϵ的概率不选择当前策略, 而是随机从**所有**动作中随机选择一个执行.


### 同策略与异策略

同策略(On Policy): 采样策略与待优化策略是同一个策略
异策略(Off Policy): 采样策略与待优化策略是不同策略


### 策略梯度

强化学习的目标是获得一个策略, 来最大化期望回报, 即求

$$\\bar{R}\_{\\theta}=\\sum\_{\\tau} R(\\tau) p\_{\\theta}(\\tau)$$

对上式计算梯度, 可得

$$\\nabla \\bar{R}\_{\\theta}=\\sum\_{\\tau} R(\\tau) \\nabla p\_{\\theta}(\\tau)=\\sum\_{\\tau} R(\\tau) p\_{\\theta}(\\tau) \\frac{\\nabla p\_{\\theta}(\\tau)}{p\_{\\theta}(\\tau)}$$

由于 \\( p\_{\\theta}(\\tau)\\)中, 可以控制的部分只有策略, 因此最终可得

$$\\nabla \\bar{R}\_{\\theta}=\\frac{1}{N} \\sum\_{n=1}^{N} \\sum\_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p\_{\\theta}\\left(a_{t}^{n} \\mid s\_{t}^{n}\\right)$$

由于要期望最大化, 因此获得梯度后, 使用梯度上升的方法, 即可逐渐增加目标函数值.