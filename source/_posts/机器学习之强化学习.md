---
title: 机器学习之强化学习
date: 2020-06-26 12:45:16
categories: 机器学习
tags:
    - 机器学习
cover_picture: images/ml.jpg
math: true
---


强化学习(Reinforcement Learning, RL), 也叫增强学习, 是指一类从(与环境)交互中不断学习的问题以及解决这类问题的方法. 强化学习的关键是贡献度分配问题, 每一个动作不能直接得到监督信息, 需要通过最终结果获得监督信息, 并且存在延时.

基础概念
--------------

强化学习涉及如下的一些基础概念
1. 智能体(agent)可以感知外界环境的状态(state)和反馈的奖励(reward),  并进行学习和决策
2. 智能体的决策功能是指根据外界环境的状态来做出不同的动作(action), 而学习功能是指根据外界环境的奖励来调整策略
3. 环境(environment)是智能体外部的所有事物, 并受智能体动作的影响而改变其状态, 并反馈给智能体相应的奖励

在此基础上, 可以抽象出如下的一些概念

名称            | 符号      | 备注
----------------|-----------|----------------------------------------------------------
状态            | s         | 对环境的描述/ 可以是离散的或连续 / 状态空间为S
动作            | a         | 对智能体行为的描述 / 可以是离散的或连续 / 动作空间为A
策略            |π(a&#124;s)     | 智能体根据环境状态s 来决定下一步的动作a 的函数
状态转移概率    | p(s′&#124;s, a) | 智能体做出一个动作, 环境在下一个时刻转变为状态s′ 的概率
即时奖励        |r(s, a, s′) | 智能体做出动作之后, 环境反馈给智能体的奖励(标量)

策略可以分为确定性策略和随机性策略, 其中随机性策略表示在某个状态下, 智能体选择的动作服从某一个概率分布而不是确定动作. 强化学习一般采用随机性策略.

### 回报与折扣率

给定一个策略后, 智能体收到的所有即时回报之和为总回报(Return). 可以引入一个折扣率来综合考虑近期回报与远期回报, 其定义为

$$G(\tau)=\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}$$

其中 \\( \gamma^{t} \\) 表示折扣率. \\( \tau \\) 表示马尔可夫决策过程的一个轨迹, 即由s,a和r构成的一组序列, 表示了智能体在环境中的一系列操作下, 相应的环境变换和奖励.


### 目标函数与值函数

强化学习的目标是获得一个策略, 来最大化期望回报, 即

$$\mathcal{J}(\theta)=E_{\tau \sim p_{\theta}(\tau)}[G(\tau)]=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=0}^{T-1} \gamma^{t} r_{t+1}\right]$$

其中\\( p_{\theta}(\tau) \\)表示一个通过参数\\(\theta\\)调节分布, 而\\(\tau \sim p_{\theta}(\tau)\\) 表示轨迹服从这一分布, 最终这个公式的含义就是此分布下求解期望值. 

上述公式可以分解为

$$\\begin{aligned}E_{\\tau \\sim p(\\tau)}[G(\\tau)] &=E_{s \\sim p\\left(s_{0}\\right)}\\left[E_{\\tau \\sim p(\\tau)}\\left[\\sum_{t=0}^{T-1} \\gamma^{t} r_{t+1} \\mid \\tau_{s_{0}}=s\\right]\\right] \\\\&=E_{s \\sim p\\left(s_{0}\\right)}\\left[V^{\\pi}(s)\\right]\\end{aligned}$$

其中\\( V^{\pi}(s) \\)称为状态值函数(State Value Function), 表示从状态s开始, 执行策略π 得到的期望总回报. 经过一番推导, 可以得到如下的贝尔曼方程, 按照这种方程计算的方法也称为动态规划法.

$$V^{\pi}(s)=E_{a \\sim \\pi(a \\mid s)} E_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$

通过此公式可以使用迭代法计算状态值函数. 如果定义状态-动作值函数为

$$Q^{\\pi}(s, a)=E_{s^{\\prime} \\sim p\\left(s^{\\prime} \\mid s, a\\right)}\\left[r\\left(s, a, s^{\\prime}\\right)+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$

则可以进一步将上述公式表示为状态值函数中\\( V^{\pi}(s) \\)是Q函数\\(Q^{\\pi}(s, a)\\)关于动作a的期望, 即

$$V^{\pi}(s)=E_{a \sim \pi(a \mid s)}\left[Q^{\pi}(s, a)\right]$$

总结两种值函数, 即
- \\(V^{\pi}(s)\\): 在状态s下, 以策略π可以获得的总期望回报
- \\(Q^{\\pi}(s, a)\\): 在状态s下, 选择动作a后, 以策略π可以获得的总期望回报

值函数可以看作是对策略π的评估. 如果在状态s, 有一个动作a使得\\( Q^{\\pi}(s, a) > V^{\pi}(s)\\), 则说明执行动作a 比当前的策略要好, 我们就可以调整参数使得动作a的概率增加.


### Q-learning核心思路

通过 **Q-table** 记录每个状态（State）下不同动作（Action）的长期预期收益（Q值），目标是找到最优策略使得总收益最大。

$$
Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]
$$

其中$\max_{a'}$部分表示计算在转移到$s'$状态后采取最佳动作能获得的最大收益. 因此根据此公式, 如果采取动作$a$后, 可以获取较大收益, 则$Q(s,a)$的值会增大.

深度强化学习
---------------


深度强化学习是指将强化学习与深度学习结合的方法. 用强化学习来定义问题和优化目标, 用深度学习来解决策略和值函数.

### 蒙特卡洛方法

模型无关的强化学习使用采样的方法计算值函数, 在这种情况下, 往往无法得知状态转移概率和及时奖励, 因此首先以状态s和动作a开始, 随机执行若干次采样过程, 获取一组轨迹, 则Q函数近似为这组轨迹的总回报的平均值

$$Q^{\\pi}(s, a) \\approx \\hat{Q}^{\\pi}(s, a)=\\frac{1}{N} \\sum_{n=1}^{N} G\\left(\\tau_{s_{0}=s, a_{0}=a}^{(n)}\\right)$$

获得近似的Q函数以后, 就可以执行策略迭代过程, 即令

$$\forall s, \pi(s)=\arg \max _{a} Q(s, a)$$

------------------

在上述蒙特卡洛方法中, 如果是确定性策略, 那么实际上采样过程获取的样本都是一样的, 并不能获取不同样本. 因此可以引入环境探索率ϵ, 使得再采样过程中, 以ϵ的概率不选择当前策略, 而是随机从**所有**动作中随机选择一个执行.


### 深度Q网络（DQN）的核心思路

用神经网络 \( Q(s,a;\theta) \) 代替Q-table，解决高维状态空间问题。DQN引入了两大创新: **经验回放（Experience Replay）**：存储转移（Transition）到缓冲区，随机抽取训练，打破数据相关性。**目标网络（Target Network）**：使用独立网络 \( Q(s,a;\theta^-) \) 计算目标Q值，缓解目标值波动问题。

损失函数定义为

$$
L = \mathbb{E}\left[ \left( r + \gamma \max_{a'} Q(s',a';\theta^-) - Q(s,a;\theta) \right)^2 \right]
$$

> 对比Q-learning的思路, 可以说是一模一样了.


### 同策略与异策略

**同策略(On Policy)**: 采样策略与待优化策略是同一个策略. **异策略(Off Policy)**: 采样策略与待优化策略是不同策略.

在深度Q网络（DQN）中，**策略网络（Policy Network）** 和 **目标网络（Target Network）** 是两个独立的神经网络，它们的引入是为了解决强化学习中的关键问题：**训练稳定性**。

在传统的Q-learning中如果直接用神经网络 \( Q(s,a;\theta) \) 替代Q-table，会出现目标值的波动性问题: 每次更新网络参数时，目标值 \( r + \gamma \max_{a'} Q(s',a';\theta) \) 也会随之变化。网络参数 \(\theta\) 的更新会导致目标值频繁波动，就像“移动靶标”一样，使训练难以稳定收敛。

DQN通过引入目标网络 \( Q(s,a;\theta^-) \) 来解决这一问题：**目标网络的参数 \(\theta^-\) 是固定的**：每隔一段时间（如每N步）从策略网络复制参数（\(\theta^- \leftarrow \theta\)）。

以PyTorch代码为例：
```python
# 策略网络（训练对象）
policy_net = DQN(state_dim, action_dim)
optimizer = optim.Adam(policy_net.parameters(), lr=0.001)

# 目标网络（参数固定，定期从策略网络复制）
target_net = DQN(state_dim, action_dim)
target_net.load_state_dict(policy_net.state_dict())  # 初始同步参数

# 训练过程中更新目标网络
if episode % TARGET_UPDATE == 0:
    target_net.load_state_dict(policy_net.state_dict())
```


### 策略梯度

强化学习的目标是获得一个策略, 来最大化期望回报, 即求

$$\\bar{R}\_{\\theta}=\\sum\_{\\tau} R(\\tau) p\_{\\theta}(\\tau)$$

对上式计算梯度, 可得

$$\\nabla \\bar{R}\_{\\theta}=\\sum\_{\\tau} R(\\tau) \\nabla p\_{\\theta}(\\tau)=\\sum\_{\\tau} R(\\tau) p\_{\\theta}(\\tau) \\frac{\\nabla p\_{\\theta}(\\tau)}{p\_{\\theta}(\\tau)}$$

由于 \\( p\_{\\theta}(\\tau)\\)中, 可以控制的部分只有策略, 因此最终可得

$$\\nabla \\bar{R}\_{\\theta}=\\frac{1}{N} \\sum\_{n=1}^{N} \\sum\_{t=1}^{T_{n}} R\\left(\\tau^{n}\\right) \\nabla \\log p\_{\\theta}\\left(a_{t}^{n} \\mid s\_{t}^{n}\\right)$$

由于要期望最大化, 因此获得梯度后, 使用梯度上升的方法, 即可逐渐增加目标函数值.

### 强化学习与游戏AI

对于绝大部分策略性不高的游戏, 使用DQN属于是大材小用. 例如对于马里奥游戏, 环境信息实际上可以认为是一个固定流程(每次重新开局都是面对同样的关卡结构和敌人), 而最佳策略也是一个几乎固定的操作步骤. 因此某种程度上来说, 这是一个可以穷举的东西.

反而是类似于围棋这样的游戏, 输入会产生非常显著的变化, 不同环境下的最佳决策也具有相当大的变化, 不能视为一个可穷举的游戏, 此时使用DQN反而具有很好的效果.

