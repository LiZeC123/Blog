---
title: 机器学习之概率论
date: 2019-01-24 09:36:35
categories: 数学
tags:
    - 机器学习
    - 概率论
cover_picture: images/ml.jpg
---
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default"></script>



Probability theory is a mathematical framework for representing uncertaion statements. It provides a means of quantifying uncertainty and axioms for deriving new uncertain statements.




边缘分布
-------------

Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as **marginal probability**.


For continuous variables, we need to use integration:

$$p(x) = \int{p(x,y)}dy$$

The name "marginal probability" comes from the process of computing marginal probabilities on paper. When the values of P(x,y) are written in a grid with different values of x in rows and different values of y in columns, it is natural to sum across a row of the grid, then write P(x) in the margin of the paper just to the right of the row.

所以边缘分布和密度函数的边缘并没有关系, 不要被这个名词误导了.

条件概率
-------------

In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a **conditional probability**. We denote the conditional probability that \\( y = y\_1\\) given \\( x = x\_1\\) as \\( P(y=y\_1|x=x\_1) \\). This conditional probability can be computed with the formula:

$$P(y=y\_1|x=x\_1) = \frac{P(y=y\_1, x=x\_1)}{P(x=x\_1)}$$

条件概率链式法则
-------------


Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:

$$P(x^{(1)}, \cdots, x^{(n)}) = P(x^{(1)}) \prod\_{i=2}^n P(x^{(i)}|x^{(1)}, \cdots, x^{(n-1)})$$

This obsservation is known as the **chain rule** or **product rule** of probability. It follows immediately from the definition of conditional probability. For example, applying the definiton twice, we get

$$
\begin{align}
 P(a,b,c) &= P(a|b,c)P(b,c) \\\\
 P(b,c)   &= P(b|c)P(c) \\\\
 P(a,b,c) &= P(a|b,c)P(b|c)P(c) \\\\
\end{align}
$$

独立和条件独立
-------------

Two random variables x and y are **independent** if their probability distribution can be expressed as a product of tow factors, one involing only x and one involving only by y:

$$\forall x \in X,y \in Y, p(x=x_0,y=y_0)=p(x=x_0)p(y=y_0)$$

Two random variables x and y are **conditionally independent** given a random variables z if the conditional probability distribution over x and f factorizes in this way for every value of z:

$$\forall x \in X,y \in Y, z \in Z,p(x=x_0,y=y_0|z=z_0)=p(x=x_0|z=z_0)p(y=y_0|z=z_0)$$

期望,方程,协方差
-----------------

The **expection** or **expected value** og some function f(x) with respect to a probability distribution P(x) is the arerage or mean value that f takes on when x is drawn from P. For continuous variables, it is computed with an integral:

$$E_x[f(x)] = \int{p(x)f(x)}dx$$

Expections are linear, for example,

$$E_x[\alpha f(x)+\beta g(x)] = \alpha E_x[f(x)]+ \beta E_x[g(x)]$$

When α and β are not dependent on x.

The **variance** gives a measure of how much the values of a function of a random variable x vary as we sample different values of x from its probability distribution:

$$Var(f(x)) = E[(f(x)-E[f(x)])^2]$$

When the variance is low, the values of f(x) vluster near their expected value. The square root of the variance is known as the **standard deviation**

The **covariance** gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:

$$Cov(f(x),g(x)) = E[(f(x)-E[f(x)])(g(x)-E[g(x)])]$$

High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. if the sign of the covariance is negative, then one variable tends to take on a relatively high value at the same times that the other takes on a relatively low values and vice versa.

The notions of covariance and dependence are related, but are in fact distince concepts. They are related because two variables that are independent have zero covariance, and two variables that have non-zero covariance are dependent. However, independence is a distinct property from covariance. For two variables to have zero covariance, there must be **no linear dependence** between them. Independence is a stronger requirement than zero covariance, because independence also excludes nonlinear relationships.


- [如何通俗易懂地解释「协方差」与「相关系数」的概念？（转）](http://blog.sina.com.cn/s/blog_6aa3b1010102xkp5.html)

<!-- 
常见概率分布
----------------

Several simple probability distributions are useful in many contexts in machine learning.

### Bernoulli分布

The **Bernolli** distribution is a distribution over a singel binary random variable. It is controlled by a single parameter \\( \phi \in [0,1] \\), which gives the probability of the random variable being queal to 1. It has the following properties:

$$P(x=1) = \phi                    \\\\
   P(x=0) = 1 - \phi                \\\\  
 P(x=x_0) = \phi ^{x_0} (1-\phi)^{1-x_0}  $$

### Multinoulli分布

### Gaussian分布

### Exponential和Laplace分布

### Dirac和Empirical分布 -->


常用函数
--------------

Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models.

### Logistic函数

One of those function is the **logistic sigmoid**:

$$\sigma(x)  = \frac{1}{1+exp(-x)}$$

The logistic sigmoid is commonly used to produce the \\( \phi \\) parameter of a Bernoulli distribution beacuse its range is (0,1), which lies within the valid range of values for the \\( \phi \\) parameter.

![The logistic sigmoid function](/images/ml/logistic.png)

### Softplus函数

Another commonly encountered function is the **softplus** function:

$$\xi(x) = log(1+exp(x))$$

The softplus function can be useful for producing the \\( \beta \\) or \\( \alpha \\) parameter of a normal distribution beacuse its range is (0, \\( \infty \\)). It also raises commonly when manipulating expressions involving sigmoids. 

The name of the softplus function comes from the fact that it is a smoothed or softened version of 

$$x^+ = max(0,x)$$

There is a graph of softplus function:

![The softplus function](/images/ml/softplus.png)

### 函数性质

These are some useful propertoes:

$$\sigma(x)$$                                              |$$\xi(x)$$
-----------------------------------------------------------|----------------------------------------------------
$$\sigma(x)  = \frac{exp(x)}{exp(x)+exp(0)}$$              |$$log \sigma(x) = -\xi(-x)$$
$$\frac{d}{dx}\sigma(x) = \sigma(x)(1-\sigma(x))$$         |$$\frac{d}{dx}\xi(x) = \sigma(x)$$
$$\forall x \in (0,1),\sigma^{-1}(x) = log(\frac{x}{1-x})$$|$$\forall x>0, \xi^{-1}(x) = log(exp(x)-1)$$
$$1 - \sigma(x) = \sigma(-x)$$                             |$$\xi(x)-\xi(-x) = x$$


贝叶斯法则
-----------

We often find overselves in a situation where we know P(y|x) and need to know P(x|y). Fortunately, if we also know P(x), we can compute the desired quantity using **Bayes' Rule**

$$P(x|y) = \frac{P(x)P(y|x)}{P(y)}$$

Note that while P(y) appears in the formula, it is usually feasible to compute 

$$P(y) = \sum_x P(y|x)P(x)$$